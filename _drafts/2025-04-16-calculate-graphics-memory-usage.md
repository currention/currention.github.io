---
title: AI - 如何计算模型微调的显存使用
date: 2025-04-16 16:00:00 +0800
categories: [AI]
tags: [AI, AI实践]
---

## 前言

在微调大模型时，显存使用是一个重要的考量因素。本文将介绍如何计算微调大模型时的显存使用。

> 在训练时，FP32每个参数占4字节，FP16占2字节。
{: .prompt-tip }


## 计算

在微调大模型时，显存使用可以分为以下几个部分：

1. 模型参数：模型参数的大小取决于模型的复杂度和参数的数量。可以使用以下公式计算模型参数的大小：

   ```
   模型参数大小 = 模型参数数量 * 模型参数类型大小
   ```

2. 梯度：梯度的大小取决于模型的复杂度和参数的数量。可以使用以下公式计算梯度的大小：

   ```
   梯度大小 = 模型参数数量 * 梯度类型大小
   ```

3. 优化器状态：激活值的大小取决于模型的复杂度和参数的数量。可以使用以下公式计算激活值的大小：

   ```
   优化器大小 = 模型参数数量 * 每个优化器状态的字节数 × 状态数
   ```

4. 激活值：激活值的大小取决于模型的复杂度和参数的数量。可以使用以下公式计算激活值的大小：

   ```
   激活值大小 = 模型参数数量 * 激活值类型大小
   激活值大小 ≈ batch_size × seq_len × hidden_size × layers × 常数

   ```

4. 其他：其他部分包括学习率调度器参数等。这些参数的大小取决于模型的复杂度和参数的数量。可以使用以下公式计算其他部分的大小：

   ```
   其他部分大小 = 其他参数数量 * 其他参数类型大小
   ```

5. 总显存使用：总显存使用等于模型参数大小、梯度大小、激活值大小和其他部分大小之和。可以使用以下公式计算总显存使用：

   ```
   总显存使用 = 模型参数大小 + 梯度大小 + 激活值大小 + 其他部分大小
   ```

## 示例


假设我们使用的是`LLaMa-7B`模型，模型参数类型`FP32`大小为4字节，梯度类型大小为4字节，激活值类型大小为4字节，其他参数类型大小为4字节。`LLaMa-7B`模型有70亿个参数，每个参数的大小为4字节，所以模型参数大小为4字节 * 70亿 = 28GB。梯度大小和激活值大小也等于28GB。假设优化器参数大小为112GB，那么总显存使用为28GB + 28GB + 28GB + 112GB = 196GB ≈ 200GB。

## 总结
通过计算模型参数大小、梯度大小、激活值大小和其他部分大小，可以计算出微调大模型时的显存使用。需要注意的是，显存使用还受到其他因素的影响，如批处理大小、优化器类型等。因此，在实际应用中，需要根据具体情况进行调整和优化。

将参数类型改为`FP16`：
- 模型参数大小：28GB → 14GB
- 梯度大小：28GB → 14GB
- 激活值大小：28GB → 14GB
- 其他部分大小：112GB → 56GB
- 总显存使用：200GB → 112GB


通过DeepSpeed的ZeRO-2/3将优化器状态、梯度分片到多卡：
- ZeRO-2：优化器状态分片，显存降低至 56GB → 28GB（2卡）​
- ZeRO-3：参数+梯度+优化器全分片，显存降低至 14GB（8卡）​